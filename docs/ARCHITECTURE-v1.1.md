# AdaApp - AI Fintech Dashboard
## å®Œæ•´æ¶æ„è®¾è®¡æŠ¥å‘Š

**ç‰ˆæœ¬**: 1.1  
**æ—¥æœŸ**: 2026-02-06  
**ä½œè€…**: James (AI Assistant)  
**æ›´æ–°**: åŠ å…¥æ•°æ®åˆ†åŒºã€å¹»è§‰æ§åˆ¶ã€è­¦æŠ¥èšåˆä¼˜åŒ–

---

## ğŸ“‹ ç›®å½•

1. [é¡¹ç›®æ¦‚è¿°](#1-é¡¹ç›®æ¦‚è¿°)
2. [ç³»ç»Ÿæ¶æ„](#2-ç³»ç»Ÿæ¶æ„)
3. [æ•°æ®åº“è®¾è®¡](#3-æ•°æ®åº“è®¾è®¡)
4. [åç«¯æœåŠ¡](#4-åç«¯æœåŠ¡)
5. [å‰ç«¯åº”ç”¨](#5-å‰ç«¯åº”ç”¨)
6. [AI Agent é›†æˆ](#6-ai-agent-é›†æˆ)
7. [è­¦æŠ¥ç³»ç»Ÿ](#7-è­¦æŠ¥ç³»ç»Ÿ)
8. [å®æ–½è®¡åˆ’](#8-å®æ–½è®¡åˆ’)
9. [æˆæœ¬ä¼°ç®—](#9-æˆæœ¬ä¼°ç®—)
10. [âš ï¸ é‡è¦ä¼˜åŒ–å»ºè®®](#10-é‡è¦ä¼˜åŒ–å»ºè®®) â† æ–°å¢

---

## 1. é¡¹ç›®æ¦‚è¿°

### 1.1 äº§å“å®šä½

AdaApp æ˜¯ä¸€ä¸ª **AI é©±åŠ¨çš„é‡‘èæ•°æ®åˆ†æå¹³å°**ï¼Œæ•´åˆå¤šç»´åº¦å¸‚åœºä¿¡å·ï¼Œå¸®åŠ©ç”¨æˆ·åšå‡ºæ›´æ˜æ™ºçš„æŠ•èµ„å†³ç­–ã€‚

### 1.2 æ ¸å¿ƒåŠŸèƒ½

| åŠŸèƒ½æ¨¡å— | æè¿° | æ•°æ®æ¥æº |
|---------|------|----------|
| ğŸ¤– AI é€‰è‚¡ | åŸºäºæŠ€æœ¯é¢/åŸºæœ¬é¢/æƒ…ç»ªçš„æ™ºèƒ½ç­›é€‰ | ç»¼åˆåˆ†æ |
| ğŸ“Š æœŸæƒå¼‚åŠ¨ | è¿½è¸ªå¤§é¢æœŸæƒäº¤æ˜“å’Œå¼‚å¸¸æ´»åŠ¨ | Options Flow API |
| ğŸ”® è´¢æŠ¥é¢„æµ‹ | é¢„æµ‹ EPS/è¥æ”¶ vs åˆ†æå¸ˆå…±è¯† | Historical + ML |
| ğŸ’¬ æƒ…ç»ªåˆ†æ | ç¤¾äº¤åª’ä½“å’Œæ–°é—»æƒ…ç»ªç›‘æ§ | Reddit/Twitter/News |
| ğŸ‘” å†…éƒ¨äº¤æ˜“ | C-suite ä¹°å–è¿½è¸ª | SEC Form 4 |
| ğŸŒ‘ Dark Pool | å¤§å®—äº¤æ˜“ç›‘æ§ | Dark Pool Feed |

### 1.3 æ ¸å¿ƒç†å¿µ

> **"ä¿¡æ¯æ‰¾äººï¼Œè€Œéäººæ‰¾ä¿¡æ¯"**

- è¢«åŠ¨æ¨¡å¼ï¼šDashboard å±•ç¤ºæ•°æ®
- ä¸»åŠ¨æ¨¡å¼ï¼šå¼‚å¸¸ä¿¡å·å®æ—¶æ¨é€åˆ° Telegram

---

## 10. âš ï¸ é‡è¦ä¼˜åŒ–å»ºè®®

> ä»¥ä¸‹æ˜¯é’ˆå¯¹ç”Ÿäº§ç¯å¢ƒçš„å…³é”®ä¼˜åŒ–ï¼Œå¿…é¡»åœ¨å¼€å‘åˆæœŸå°±è€ƒè™‘ã€‚

### 10.1 æ•°æ®é‡ä¸ç´¢å¼•ä¼˜åŒ–ï¼ˆè¡¨åˆ†åŒºï¼‰

**é—®é¢˜**: `options_flow` å’Œ `dark_pool_orders` åœ¨äº¤æ˜“é«˜å³°æœŸæ•°æ®é‡æå…¶åºå¤§ï¼ŒæŸ¥è¯¢å†å²æ•°æ®ä¼šå˜æ…¢ã€‚

**è§£å†³æ–¹æ¡ˆ**: PostgreSQL æ—¶é—´åˆ†åŒº

```sql
-- ================================================
-- æœŸæƒå¼‚åŠ¨è¡¨ - æŒ‰æœˆåˆ†åŒº
-- ================================================

-- åˆ›å»ºåˆ†åŒºçˆ¶è¡¨
CREATE TABLE options_flow (
    id UUID DEFAULT gen_random_uuid(),
    symbol VARCHAR(10) NOT NULL,
    option_type VARCHAR(4) NOT NULL,
    strike DECIMAL(12,2) NOT NULL,
    expiration DATE NOT NULL,
    premium DECIMAL(15,2) NOT NULL,
    volume INTEGER NOT NULL,
    open_interest INTEGER,
    implied_volatility DECIMAL(6,4),
    unusual_score DECIMAL(5,2),
    trade_type VARCHAR(20),
    sentiment VARCHAR(10),
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    PRIMARY KEY (id, created_at)  -- åˆ†åŒºé”®å¿…é¡»åŒ…å«åœ¨ä¸»é”®ä¸­
) PARTITION BY RANGE (created_at);

-- åˆ›å»ºæœˆåº¦åˆ†åŒº
CREATE TABLE options_flow_2026_01 PARTITION OF options_flow
    FOR VALUES FROM ('2026-01-01') TO ('2026-02-01');
CREATE TABLE options_flow_2026_02 PARTITION OF options_flow
    FOR VALUES FROM ('2026-02-01') TO ('2026-03-01');
CREATE TABLE options_flow_2026_03 PARTITION OF options_flow
    FOR VALUES FROM ('2026-03-01') TO ('2026-04-01');
-- ... ä»¥æ­¤ç±»æ¨

-- è‡ªåŠ¨åˆ›å»ºåˆ†åŒºçš„å‡½æ•°ï¼ˆå¯ç”¨ pg_cron è°ƒåº¦ï¼‰
CREATE OR REPLACE FUNCTION create_monthly_partition()
RETURNS void AS $$
DECLARE
    partition_date DATE;
    partition_name TEXT;
    start_date TEXT;
    end_date TEXT;
BEGIN
    partition_date := DATE_TRUNC('month', NOW() + INTERVAL '1 month');
    partition_name := 'options_flow_' || TO_CHAR(partition_date, 'YYYY_MM');
    start_date := TO_CHAR(partition_date, 'YYYY-MM-DD');
    end_date := TO_CHAR(partition_date + INTERVAL '1 month', 'YYYY-MM-DD');
    
    EXECUTE format(
        'CREATE TABLE IF NOT EXISTS %I PARTITION OF options_flow 
         FOR VALUES FROM (%L) TO (%L)',
        partition_name, start_date, end_date
    );
END;
$$ LANGUAGE plpgsql;

-- ================================================
-- Dark Pool è¡¨ - åŒæ ·æŒ‰æœˆåˆ†åŒº
-- ================================================

CREATE TABLE dark_pool_orders (
    id UUID DEFAULT gen_random_uuid(),
    symbol VARCHAR(10) NOT NULL,
    price DECIMAL(12,4) NOT NULL,
    size INTEGER NOT NULL,
    value DECIMAL(15,2) GENERATED ALWAYS AS (price * size) STORED,
    exchange VARCHAR(20),
    trade_time TIMESTAMP WITH TIME ZONE,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    PRIMARY KEY (id, created_at)
) PARTITION BY RANGE (created_at);

-- åˆ†åŒºç­–ç•¥åŒä¸Š...
```

**åˆ†åŒºä¼˜åŠ¿**:
- æŸ¥è¯¢æœ€è¿‘æ•°æ®æ—¶åªæ‰«æç›¸å…³åˆ†åŒº
- å†å²æ•°æ®å¯å½’æ¡£æˆ–åˆ é™¤æ•´ä¸ªåˆ†åŒº
- ç´¢å¼•ä½“ç§¯æ›´å°ï¼Œç»´æŠ¤æ›´å¿«

**Supabase æ³¨æ„äº‹é¡¹**:
- Supabase æ”¯æŒåˆ†åŒºè¡¨ï¼Œä½†éœ€è¦åœ¨ SQL Editor ä¸­æ‰‹åŠ¨åˆ›å»º
- RLS ç­–ç•¥éœ€è¦åœ¨çˆ¶è¡¨ä¸Šè®¾ç½®ï¼Œä¼šè‡ªåŠ¨ç»§æ‰¿åˆ°åˆ†åŒº

---

### 10.2 AI å¹»è§‰æ§åˆ¶ï¼ˆStructured Outputsï¼‰

**é—®é¢˜**: é‡‘èæ•°æ®æåº¦æ•æ„Ÿï¼ŒAI å¯èƒ½åœ¨åˆ†ææŠ¥å‘Šä¸­"ç¼–é€ "æ•°æ®ã€‚

**æ ¸å¿ƒåŸåˆ™**: 
> **AI åªè´Ÿè´£å½’çº³åˆ†æï¼Œä¸è´Ÿè´£è®¡ç®—ã€‚æ‰€æœ‰æ•°å­—ç”±ä»£ç è®¡ç®—åä¼ ç»™ AIã€‚**

**è§£å†³æ–¹æ¡ˆ**: å¼ºåˆ¶ä½¿ç”¨ JSON Schema çº¦æŸè¾“å‡º

```python
# agents/stock_selector.py

from pydantic import BaseModel, Field
from typing import Literal, List

# å®šä¹‰ç»“æ„åŒ–è¾“å‡º Schema
class AnalysisFactor(BaseModel):
    """å•ä¸ªåˆ†æå› ç´ """
    name: str = Field(description="å› ç´ åç§°ï¼Œå¦‚'RSIè¶…å–'")
    impact: Literal["positive", "negative", "neutral"]
    weight: float = Field(ge=0, le=1, description="æƒé‡ 0-1ï¼Œç”±ä»£ç é¢„è®¡ç®—")

class StockAnalysis(BaseModel):
    """AI åˆ†æè¾“å‡ºçš„ä¸¥æ ¼æ ¼å¼"""
    summary: str = Field(
        max_length=200, 
        description="ä¸€å¥è¯æ€»ç»“ï¼Œä¸è¦åŒ…å«ä»»ä½•æ•°å­—"
    )
    reasoning: str = Field(
        max_length=500,
        description="åˆ†æé€»è¾‘ï¼Œå¼•ç”¨æˆ‘æä¾›çš„æ•°æ®ï¼Œä¸è¦è‡ªå·±è®¡ç®—"
    )
    key_factors: List[AnalysisFactor] = Field(
        max_items=5,
        description="å…³é”®å› ç´ ï¼Œæ‰€æœ‰æ•°å€¼ç”±æˆ‘æä¾›"
    )
    risk_notes: str = Field(
        max_length=200,
        description="é£é™©æç¤º"
    )

async def generate_analysis(self, symbol: str, data: dict) -> StockAnalysis:
    """
    è°ƒç”¨ AI ç”Ÿæˆåˆ†æï¼Œä½¿ç”¨ Structured Output
    """
    
    # 1. æ‰€æœ‰æ•°å­—ç”± Python é¢„è®¡ç®—
    context = f"""
    åˆ†æä»¥ä¸‹è‚¡ç¥¨æ•°æ®ï¼Œåªä½¿ç”¨æˆ‘æä¾›çš„æ•°å­—ï¼Œä¸è¦è‡ªå·±è®¡ç®—ä»»ä½•æ•°å€¼ï¼š
    
    è‚¡ç¥¨: {symbol}
    
    ## é¢„è®¡ç®—æ•°æ®ï¼ˆç›´æ¥å¼•ç”¨ï¼Œä¸è¦ä¿®æ”¹ï¼‰
    - å½“å‰ä»·æ ¼: ${data['price']:.2f}
    - 5æ—¥æ¶¨è·Œ: {data['change_5d']:+.2f}%
    - RSI(14): {data['rsi']:.1f}
    - æœŸæƒ Put/Call æ¯”: {data['pcr']:.2f}
    - æƒ…ç»ªåˆ†æ•°: {data['sentiment']:+.2f}
    - å†…éƒ¨äººå£«æœ¬æœˆä¹°å…¥: ${data['insider_buys']:,.0f}
    - ç»¼åˆå¾—åˆ†: {data['composite_score']:.2f} (ç”±ç³»ç»Ÿè®¡ç®—)
    
    ## ä½ çš„ä»»åŠ¡
    1. æ ¹æ®ä»¥ä¸Šæ•°æ®å†™ä¸€æ®µåˆ†æ
    2. ä¸è¦ç¼–é€ ä»»ä½•æ•°å­—
    3. ä¸è¦è®¡ç®—ä»»ä½•ç™¾åˆ†æ¯”
    4. å¼•ç”¨æ•°æ®æ—¶ä½¿ç”¨æˆ‘æä¾›çš„åŸå§‹å€¼
    """
    
    # 2. ä½¿ç”¨ OpenAI/Anthropic çš„ Structured Output
    response = await client.chat.completions.create(
        model="gpt-4o",
        messages=[{"role": "user", "content": context}],
        response_format={
            "type": "json_schema",
            "json_schema": {
                "name": "stock_analysis",
                "schema": StockAnalysis.model_json_schema()
            }
        }
    )
    
    # 3. è§£æå¹¶éªŒè¯
    analysis = StockAnalysis.model_validate_json(response.choices[0].message.content)
    
    return analysis
```

**å¯¹äº Claude (Anthropic)**:

```python
# ä½¿ç”¨ tool_use æ¨¡å¼å¼ºåˆ¶ç»“æ„åŒ–è¾“å‡º
response = await anthropic.messages.create(
    model="claude-sonnet-4-5",
    max_tokens=1024,
    tools=[{
        "name": "submit_analysis",
        "description": "æäº¤è‚¡ç¥¨åˆ†æç»“æœ",
        "input_schema": StockAnalysis.model_json_schema()
    }],
    tool_choice={"type": "tool", "name": "submit_analysis"},
    messages=[{"role": "user", "content": context}]
)

# ä» tool_use ä¸­æå–ç»“æœ
analysis = StockAnalysis.model_validate(response.content[0].input)
```

**éªŒè¯å±‚**:

```python
# utils/validators.py

def validate_analysis(analysis: StockAnalysis, source_data: dict) -> bool:
    """
    äºŒæ¬¡éªŒè¯ï¼šç¡®ä¿ AI è¾“å‡ºæ²¡æœ‰ç¼–é€ æ•°å­—
    """
    text = analysis.summary + analysis.reasoning
    
    # æ£€æŸ¥æ˜¯å¦åŒ…å«æœªæä¾›çš„æ•°å­—
    import re
    numbers_in_text = re.findall(r'\d+\.?\d*%?', text)
    
    allowed_numbers = {
        str(source_data['price']),
        f"{source_data['change_5d']:.2f}",
        f"{source_data['rsi']:.1f}",
        # ... æ‰€æœ‰å…è®¸çš„æ•°å­—
    }
    
    for num in numbers_in_text:
        if num not in allowed_numbers and float(num.rstrip('%')) > 1:
            logging.warning(f"AI å¯èƒ½ç¼–é€ äº†æ•°å­—: {num}")
            return False
    
    return True
```

---

### 10.3 è­¦æŠ¥ç–²åŠ³ç®¡ç†ï¼ˆèšåˆé€»è¾‘ï¼‰

**é—®é¢˜**: å¸‚åœºæ³¢åŠ¨å¤§æ—¶ï¼Œå¯èƒ½ä¸€åˆ†é’Ÿå‘ 50 æ¡æ¶ˆæ¯ï¼Œç”¨æˆ·ä¼šå…³æ‰é€šçŸ¥ã€‚

**è§£å†³æ–¹æ¡ˆ**: è­¦æŠ¥èšåˆ + å†·å´æœŸ + ä¼˜å…ˆçº§é˜Ÿåˆ—

```python
# alerting/alert_aggregator.py

from dataclasses import dataclass, field
from datetime import datetime, timedelta
from collections import defaultdict
import asyncio

@dataclass
class Alert:
    symbol: str
    alert_type: str
    priority: str  # 'high', 'medium', 'low'
    data: dict
    timestamp: datetime = field(default_factory=datetime.utcnow)

class AlertAggregator:
    """
    è­¦æŠ¥èšåˆå™¨
    - ç›¸åŒè‚¡ç¥¨çš„å¤šä¸ªè­¦æŠ¥åœ¨æ—¶é—´çª—å£å†…åˆå¹¶
    - æ ¹æ®ä¼˜å…ˆçº§å†³å®šå‘é€ç­–ç•¥
    - å…¨å±€å†·å´æœŸé˜²æ­¢åˆ·å±
    """
    
    def __init__(
        self,
        aggregation_window: int = 300,  # 5åˆ†é’Ÿèšåˆçª—å£
        cooldown_per_symbol: int = 600,  # æ¯è‚¡ç¥¨10åˆ†é’Ÿå†·å´
        max_alerts_per_minute: int = 5,  # æ¯åˆ†é’Ÿæœ€å¤š5æ¡
    ):
        self.aggregation_window = aggregation_window
        self.cooldown_per_symbol = cooldown_per_symbol
        self.max_alerts_per_minute = max_alerts_per_minute
        
        self.pending_alerts: dict[str, list[Alert]] = defaultdict(list)
        self.last_sent: dict[str, datetime] = {}
        self.sent_this_minute: int = 0
        self.minute_reset: datetime = datetime.utcnow()
        
        self._lock = asyncio.Lock()
    
    async def add_alert(self, alert: Alert) -> None:
        """æ·»åŠ è­¦æŠ¥åˆ°èšåˆé˜Ÿåˆ—"""
        async with self._lock:
            key = f"{alert.symbol}:{alert.alert_type}"
            self.pending_alerts[key].append(alert)
    
    async def process_alerts(self) -> list[dict]:
        """
        å¤„ç†èšåˆé˜Ÿåˆ—ï¼Œè¿”å›è¦å‘é€çš„æ¶ˆæ¯
        æ¯éš”ä¸€å®šæ—¶é—´è°ƒç”¨ä¸€æ¬¡
        """
        async with self._lock:
            now = datetime.utcnow()
            
            # é‡ç½®æ¯åˆ†é’Ÿè®¡æ•°å™¨
            if (now - self.minute_reset).seconds >= 60:
                self.sent_this_minute = 0
                self.minute_reset = now
            
            messages_to_send = []
            keys_to_clear = []
            
            for key, alerts in self.pending_alerts.items():
                if not alerts:
                    continue
                
                symbol = alerts[0].symbol
                oldest = min(a.timestamp for a in alerts)
                
                # æ£€æŸ¥æ˜¯å¦åœ¨èšåˆçª—å£å†…
                if (now - oldest).seconds < self.aggregation_window:
                    # è¿˜åœ¨èšåˆä¸­ï¼Œé™¤éæ˜¯é«˜ä¼˜å…ˆçº§
                    if not any(a.priority == 'high' for a in alerts):
                        continue
                
                # æ£€æŸ¥å†·å´æœŸ
                if symbol in self.last_sent:
                    if (now - self.last_sent[symbol]).seconds < self.cooldown_per_symbol:
                        # å†·å´ä¸­ï¼Œé™¤éæ˜¯é«˜ä¼˜å…ˆçº§
                        if not any(a.priority == 'high' for a in alerts):
                            continue
                
                # æ£€æŸ¥æ¯åˆ†é’Ÿé™åˆ¶
                if self.sent_this_minute >= self.max_alerts_per_minute:
                    # åªè®©é«˜ä¼˜å…ˆçº§é€šè¿‡
                    if not any(a.priority == 'high' for a in alerts):
                        continue
                
                # ç”Ÿæˆèšåˆæ¶ˆæ¯
                message = self._aggregate_message(alerts)
                messages_to_send.append(message)
                
                # æ›´æ–°çŠ¶æ€
                self.last_sent[symbol] = now
                self.sent_this_minute += 1
                keys_to_clear.append(key)
            
            # æ¸…ç†å·²å¤„ç†çš„è­¦æŠ¥
            for key in keys_to_clear:
                self.pending_alerts[key] = []
            
            return messages_to_send
    
    def _aggregate_message(self, alerts: list[Alert]) -> dict:
        """å°†å¤šä¸ªè­¦æŠ¥åˆå¹¶æˆä¸€æ¡æ¶ˆæ¯"""
        
        symbol = alerts[0].symbol
        count = len(alerts)
        
        if count == 1:
            # å•æ¡è­¦æŠ¥ï¼Œç›´æ¥è¿”å›
            return self._format_single_alert(alerts[0])
        
        # å¤šæ¡è­¦æŠ¥ï¼Œç”Ÿæˆèšåˆæ¶ˆæ¯
        alert_types = set(a.alert_type for a in alerts)
        highest_priority = 'high' if any(a.priority == 'high' for a in alerts) else 'medium'
        
        # æ±‡æ€»æ•°æ®
        total_value = sum(a.data.get('value', 0) for a in alerts)
        
        message = f"""
ğŸ”” **{symbol} å¤šé‡ä¿¡å·èšåˆ** ({count} æ¡è­¦æŠ¥)

**è§¦å‘ç±»å‹**:
{self._format_alert_types(alerts)}

**å…³é”®æ•°æ®**:
- ç´¯è®¡é‡‘é¢: ${total_value:,.2f}
- æ—¶é—´è·¨åº¦: {self._format_time_span(alerts)}

**å»ºè®®**: å¤šé‡ä¿¡å·å åŠ ï¼Œå»ºè®®é‡ç‚¹å…³æ³¨
        """.strip()
        
        return {
            "symbol": symbol,
            "message": message,
            "priority": highest_priority,
            "alert_count": count
        }
    
    def _format_alert_types(self, alerts: list[Alert]) -> str:
        """æ ¼å¼åŒ–è­¦æŠ¥ç±»å‹åˆ—è¡¨"""
        type_counts = defaultdict(int)
        for a in alerts:
            type_counts[a.alert_type] += 1
        
        lines = []
        type_emojis = {
            'insider_buy': 'ğŸ‘” å†…éƒ¨ä¹°å…¥',
            'options_unusual': 'ğŸ“Š æœŸæƒå¼‚åŠ¨',
            'dark_pool': 'ğŸŒ‘ Dark Pool',
            'sentiment_spike': 'ğŸ’¬ æƒ…ç»ªå¼‚åŠ¨',
            'ai_signal': 'ğŸ¤– AI ä¿¡å·'
        }
        
        for alert_type, count in type_counts.items():
            emoji_name = type_emojis.get(alert_type, alert_type)
            lines.append(f"  â€¢ {emoji_name} x{count}")
        
        return '\n'.join(lines)
    
    def _format_time_span(self, alerts: list[Alert]) -> str:
        """æ ¼å¼åŒ–æ—¶é—´è·¨åº¦"""
        times = [a.timestamp for a in alerts]
        span = max(times) - min(times)
        minutes = span.seconds // 60
        return f"{minutes} åˆ†é’Ÿå†…"

# ä½¿ç”¨ç¤ºä¾‹
aggregator = AlertAggregator(
    aggregation_window=300,     # 5åˆ†é’Ÿèšåˆ
    cooldown_per_symbol=600,    # æ¯è‚¡ç¥¨10åˆ†é’Ÿå†·å´
    max_alerts_per_minute=5     # æ¯åˆ†é’Ÿæœ€å¤š5æ¡
)

# åå°ä»»åŠ¡ï¼šæ¯30ç§’å¤„ç†ä¸€æ¬¡èšåˆé˜Ÿåˆ—
async def alert_processor():
    while True:
        messages = await aggregator.process_alerts()
        for msg in messages:
            await telegram_notifier.send(msg)
        await asyncio.sleep(30)
```

**é…ç½®å»ºè®®**:

| åœºæ™¯ | èšåˆçª—å£ | å†·å´æœŸ | æ¯åˆ†é’Ÿä¸Šé™ |
|-----|---------|-------|----------|
| æ¿€è¿›å‹ | 2åˆ†é’Ÿ | 5åˆ†é’Ÿ | 10æ¡ |
| å¹³è¡¡å‹ï¼ˆæ¨èï¼‰ | 5åˆ†é’Ÿ | 10åˆ†é’Ÿ | 5æ¡ |
| ä¿å®ˆå‹ | 15åˆ†é’Ÿ | 30åˆ†é’Ÿ | 3æ¡ |

---

### 10.4 Python Agents éƒ¨ç½²æ–¹æ¡ˆ

**é—®é¢˜**: å¤šä¸ª Python Agents éœ€è¦ç¨³å®šçš„å®šæ—¶è¿è¡Œç¯å¢ƒã€‚

**è§£å†³æ–¹æ¡ˆ**: 3 ç§æ–¹æ¡ˆæŒ‰å¤æ‚åº¦æ’åº

#### æ–¹æ¡ˆ A: ç›´æ¥ Cronï¼ˆç®€å•ï¼‰

```bash
# /etc/cron.d/adaapp

# æ¯5åˆ†é’Ÿè¿è¡ŒæœŸæƒç›‘æ§
*/5 * * * * root cd /root/adaapp && /usr/bin/python3 -m agents.options_watcher >> /var/log/adaapp/options.log 2>&1

# æ¯å°æ—¶è¿è¡Œæƒ…ç»ªåˆ†æ
0 * * * * root cd /root/adaapp && /usr/bin/python3 -m agents.sentiment_analyzer >> /var/log/adaapp/sentiment.log 2>&1

# æ¯å¤©9:30è¿è¡Œé€‰è‚¡ï¼ˆç¾ä¸œå¼€ç›˜å‰ï¼‰
30 13 * * 1-5 root cd /root/adaapp && /usr/bin/python3 -m agents.stock_selector >> /var/log/adaapp/selector.log 2>&1

# æ¯å¤©18:00è¿è¡Œå†…éƒ¨äº¤æ˜“æ£€æŸ¥ï¼ˆSEC Form 4 é€šå¸¸ä¸‹åˆå‘å¸ƒï¼‰
0 22 * * 1-5 root cd /root/adaapp && /usr/bin/python3 -m agents.insider_tracker >> /var/log/adaapp/insider.log 2>&1
```

#### æ–¹æ¡ˆ B: APScheduler + Systemdï¼ˆæ¨èï¼‰

```python
# scheduler/main.py

from apscheduler.schedulers.asyncio import AsyncIOScheduler
from apscheduler.triggers.cron import CronTrigger
import asyncio

scheduler = AsyncIOScheduler()

# æœŸæƒç›‘æ§ - æ¯5åˆ†é’Ÿ
scheduler.add_job(
    options_watcher.run,
    CronTrigger(minute='*/5'),
    id='options_watcher',
    name='Options Flow Watcher'
)

# æƒ…ç»ªåˆ†æ - æ¯å°æ—¶
scheduler.add_job(
    sentiment_analyzer.run,
    CronTrigger(minute=0),
    id='sentiment_analyzer',
    name='Sentiment Analyzer'
)

# é€‰è‚¡ - æ¯å¤©9:30 ET (13:30 UTC)
scheduler.add_job(
    stock_selector.run,
    CronTrigger(hour=13, minute=30, day_of_week='mon-fri'),
    id='stock_selector',
    name='Stock Selector'
)

# è­¦æŠ¥å¤„ç† - æ¯30ç§’
scheduler.add_job(
    alert_processor.run,
    'interval',
    seconds=30,
    id='alert_processor',
    name='Alert Processor'
)

if __name__ == '__main__':
    scheduler.start()
    asyncio.get_event_loop().run_forever()
```

Systemd æœåŠ¡:

```ini
# /etc/systemd/system/adaapp-scheduler.service

[Unit]
Description=AdaApp Agent Scheduler
After=network.target

[Service]
Type=simple
User=root
WorkingDirectory=/root/adaapp
ExecStart=/usr/bin/python3 -m scheduler.main
Restart=always
RestartSec=10
Environment=PYTHONPATH=/root/adaapp

[Install]
WantedBy=multi-user.target
```

#### æ–¹æ¡ˆ C: Docker Composeï¼ˆå®Œæ•´ï¼‰

```yaml
# docker-compose.yml

version: '3.8'

services:
  scheduler:
    build: .
    command: python -m scheduler.main
    restart: always
    environment:
      - SUPABASE_URL=${SUPABASE_URL}
      - SUPABASE_KEY=${SUPABASE_KEY}
      - OPENCLAW_URL=http://host.docker.internal:18789
    volumes:
      - ./logs:/app/logs
    
  api:
    build: .
    command: uvicorn api.main:app --host 0.0.0.0 --port 8000
    ports:
      - "8000:8000"
    restart: always
    environment:
      - SUPABASE_URL=${SUPABASE_URL}
      - SUPABASE_KEY=${SUPABASE_KEY}
```

---

### 10.5 ä¼˜åŒ–æ¸…å•æ€»ç»“

| ä¼˜åŒ–é¡¹ | ä¼˜å…ˆçº§ | å¤æ‚åº¦ | é˜¶æ®µ |
|-------|-------|-------|------|
| è¡¨åˆ†åŒº | é«˜ | ä¸­ | Phase 1 |
| AI å¹»è§‰æ§åˆ¶ | é«˜ | ä½ | Phase 2 |
| è­¦æŠ¥èšåˆ | é«˜ | ä¸­ | Phase 5 |
| Scheduler éƒ¨ç½² | ä¸­ | ä½ | Phase 2 |
| æ—¥å¿—ç›‘æ§ | ä¸­ | ä½ | Phase 6 |

---

*æ–‡æ¡£ç‰ˆæœ¬ 1.1 - å·²æ•´åˆç”Ÿäº§ç¯å¢ƒä¼˜åŒ–å»ºè®®*
